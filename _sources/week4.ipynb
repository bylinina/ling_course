{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4. Grammar I: Morphology\n",
    "\n",
    "`````{admonition} TL;DR [[slides](https://docs.google.com/presentation/d/1USbvMiLbEx-GCgd5MS_OioFiVJO0A6G9f9xzKKiDWK8/edit?usp=sharing)]\n",
    ":class: note\n",
    "- **Morphology** as a study of words and their parts, **morphemes**.\n",
    "- Word vs. morpheme: a sometimes complicated distinction\n",
    "- Other traditional distinctions: \n",
    "    - Free vs. bound morphemes; \n",
    "    - Roots vs. affixes; \n",
    "    - Open- vs. closed-class morphemes;\n",
    "    - Derivational vs. inflectional morphology\n",
    "- Positional types of affixes\n",
    "- Morphological types of languages: fusion and syncretism\n",
    "- Morphology and language technology\n",
    "`````\n",
    "\n",
    "\n",
    "`````{admonition} Source acknowledgement\n",
    ":class: note, dropdown\n",
    "This class follows the logic and general structure that's a mix from [MIT undergrad Intro to Linguistics](https://ocw.mit.edu/courses/24-900-introduction-to-linguistics-spring-2022/), [CMU Multilingual NLP course](http://demo.clab.cs.cmu.edu/11737fa20/) morphology lectures. Some examples are taken from these sources, some other examples come from _Kibrik et al. 2019. Introduction to Language Science (in Russian)_ and _Plungian. 2003. General morphology (in Russian)_.\n",
    "`````\n",
    "\n",
    "\n",
    "\n",
    "## Words and morphemes\n",
    "\n",
    "This week we are moving from the immediately observable side of language -- its sound or other modalities -- to its deeper, not immediately visible organization: grammar. We will be looking at how words are put together into phrases and sentences (syntax), and how words are built from even smaller parts (morphology). We start with morphology. \n",
    "\n",
    "`````{admonition} Important notion\n",
    ":class: warning\n",
    "**Morphology** studies words and their internal structure.  \n",
    "`````\n",
    "\n",
    "Intuitively, we all know what **words** are. In fact, knowing the words of a language -- and consequently, being able to tell a word from a non-word or from a bigger or smaller unit -- is part of knowing the language. A typical word has a bunch of properties:\n",
    "\n",
    "- In written text, it's separated from other words by white spaces.\n",
    "- In speech, it has one main stress.\n",
    "- It describes a single idea / concept.\n",
    "- It has autonomy that smaller units don't have: it can be an utterance by itself (for example, as an answer to a question) and can occupy different positions in the sentence, not glued to any other word.\n",
    "\n",
    "Together, these properties outline very clear words. But we should be careful relying on these particular properties as definitions. Let's look at the sentence below:\n",
    "\n",
    "<center><big><span style=\"color:DeepPink\"><b>Juniper</b></span><span style=\"color:Indigo\"><b>'s</b></span> <span style=\"color:Crimson\"><b>mother-in-law</b></span> <span style=\"color:FireBrick\"><b>is</b></span> <span style=\"color:DarkSlateGray\"><b>kind</b></span><span style=\"color:Purple\"><b>a</b></span> <span style=\"color:SteelBlue\"><b>hilarious</b></span>, <span style=\"color:DarkGreen\"><b>is</b></span><span style=\"color:DarkRed\"><b>n't</b></span> <span style=\"color:ForestGreen\"><b>she</b></span>?</big></center>\n",
    "\n",
    "<br>\n",
    "Some words  clear -- <span style=\"color:FireBrick\"><b>is</b></span>, <span style=\"color:SteelBlue\"><b>hilarious</b></span> and <span style=\"color:ForestGreen\"><b>she</b></span> are definitely words according to all the properties above -- apart from maybe <span style=\"color:ForestGreen\"><b>she</b></span> and <span style=\"color:FireBrick\"><b>is</b></span>, not so sure what the concepts behind them are. But what about the rest? Those are not obvious.\n",
    "\n",
    "- Is <span style=\"color:DarkSlateGray\"><b>kind</b></span><span style=\"color:Purple\"><b>a</b></span> one word -- or does the fact that the full version -- _kind of_ -- is spelled with a space between its parts make this decision suspicious?\n",
    "- What about <span style=\"color:Crimson\"><b>mother-in-law</b></span>  -- one word or three?\n",
    "- Same question -- one word or more? -- for <span style=\"color:DeepPink\"><b>Juniper</b></span><span style=\"color:Indigo\"><b>'s</b></span> and <span style=\"color:DarkGreen\"><b>is</b></span><span style=\"color:DarkRed\"><b>n't</b></span>.\n",
    "\n",
    "In these -- and other -- more complicated cases not all of these criteria are equally helpful. \n",
    "- The general down side of the **orthographic** criterion is that it makes sense only for written languages -- and, as we discussed before, only half of languages are written. Certainly the other half has words too!\n",
    "- The **semantic** definition is not very reliable either -- at least until we have a better definition what a single idea is and how to tell it from a combination of ideas or parts of ideas (recall that we mentioned this problem before when discussing whether it's realistic to have a purely **ideographic** writing system).\n",
    "- The **phonetic** and **distributional** criteria are more widely applicable. They can say something interesting in problematic cases! For example, <span style=\"color:Indigo\"><b>'s</b></span> and <span style=\"color:DarkRed\"><b>n't</b></span> can't carry their own stress -- simply because they have no vowel to put it on -- and so they are not autonomous in that they can't appear on their own. At the same time, <span style=\"color:Indigo\"><b>'s</b></span> has freedom that <span style=\"color:DarkRed\"><b>n't</b></span> doesn't: it can be separated pretty far from the word it's related to (_The guy from the gym's phone number_ is a well-formed sentence in English), while <span style=\"color:DarkRed\"><b>n't</b></span> is stuck right next to <span style=\"color:DarkGreen\"><b>is</b></span>.\n",
    "\n",
    "Language is full of intermediate cases like <span style=\"color:Indigo\"><b>'s</b></span> and others (often referred to as **clitics** in the literature). In some cases, there is no obvious answer whether something is a word or not. The vagueness of this boundary between words and units of other sizes corresponds to the vagueness of the division of labor between morphology and syntax. In some cases, a closer look would help us make this decision, but in other cases it will only highlight the true complexity of this boundary. We will do our best to be as precise as possible where we can! What's important here is the general intuition that the connections between **morphemes** within a word are generally more tight than the connections between words in a sentence (the latter are studied by **syntax**, or next topic.)\n",
    "\n",
    "\n",
    "`````{admonition} Important notion\n",
    ":class: warning\n",
    "**Morphemes** are smallest meaningful linguistic units.\n",
    "`````\n",
    "\n",
    "Why do we need to say 'meaningful' in this definition? This is to differentiate them from other units that we studied before -- sounds and syllables, the building blocks that language uses to make units that grammar operates with -- pairings of sound and meaning. This is what morphemes are.\n",
    "\n",
    "It's worth noting here that meanings of some morphemes are easier to pin down than others. When talking about morphemes as meaningful units, you will often see these kinds of diagrams:\n",
    "\n",
    "```{margin} \n",
    "Image of a cat generated by Midjourney.\n",
    "```\n",
    "```{image} ./images/cat.jpg\n",
    ":alt: morpheme 'cat'\n",
    ":class: bg-primary mb-1\n",
    ":width: 350px\n",
    ":align: center\n",
    "```\n",
    "\n",
    "But nobody draws something similar for _'s_, _-ed_ or _-ity_. Do those morphemes even have meanings? They do, but their meanings are much harder to formulate. We will talk about them during Week 6.\n",
    "\n",
    "Let's move on to some notions and distinctions important in the field of morphology. Almost none of them are clear-cut or even make obvious sense. I'm not going to pretend they do when they don't -- but we need to know what people talk about when they use these terms, so hold tight.\n",
    "\n",
    "### Free vs. bound; roots vs. affixes; open- vs. closed-class\n",
    "\n",
    "\n",
    "The first distinction between different types of morphemes that is often drawn is free morphemes vs. bound morphemes. \n",
    "\n",
    "`````{admonition} Important notions\n",
    ":class: warning\n",
    "- **Free morphemes** can constitute words by themselves, without any other morphemes.  \n",
    "- **Bound morphemes** are never words by themselves but are always parts of words.\n",
    "`````\n",
    "\n",
    "When we say a morpheme is free and can be a word by itself, we mean that it has the autonomy properties a word has, including the power to be used in isolation as an utterance (but keep in mind the caveat in the previous section!) and/or to be linearly separated from the item it's related to. \n",
    "\n",
    "Examples of free morphemes are typically **roots** -- _cat_, _nice_, _walk_ and so on; examples of bound morphemes are usually **affixes** like _-ed_, _anti-_, _-less_ etc. \n",
    "\n",
    "Don't these distinctions then describe the same thing? Not really. In a language with not much morphology -- like English -- it is tempting to just say that roots are always free and affixes are always bound, but in languages that require you to specify a lot of grammatical information on most words in any context, you don't often see roots without affixes expressing this information attached. In those languages, the distinctions free vs. bound and roots vs. affixes are misaligned. Well, affixes are still bound, but roots -- a lot of them! -- are bound too. For instance, in Latin, a lot of nouns always come with non-empty case and number markers, so you never see the root _lūn-_ 'moon' without some other morpheme accompanying it. According to the definition above, this root then is bound rather than free.\n",
    "\n",
    "```{margin} \n",
    "Latin\n",
    "```\n",
    "|     |   **Sg**   | **Pl**       |\n",
    "|:---:|:------|:----------|\n",
    "| **Nom** | lūn-a  | lūn-ae   |\n",
    "| **Gen** | lūn-ae | lūn-ārum |\n",
    "| **Dat** | lūn-ae | lūn-īs   |\n",
    "| **Abl** | lūn-ā  | lūn-īs   |\n",
    "\n",
    "\n",
    "Let's say something slightly more accurate: \n",
    "\n",
    "<center>Every word contains at least one root.</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "There are words that contain more than one root -- these are compounds like _toothbrush_, for example. There are words that contain just the root -- for instance, _tooth_. There are words that contain a root and an affix, as in _brush-ing_. But there are no words that only contain affixes. \n",
    "\n",
    "This is helpful in more accurately connecting the root vs. affix distinction to the distinction between free vs. bound morphemes. But is this really what roots are? Things that every word has?\n",
    "\n",
    "Maybe that's all we can really say. Sometimes, you will see that roots are defined **semantically**, as a morpheme that 'carries the main content of the word'. As it's often the case with strategies of reducing a complex linguistic construct to meaning-based properties, it works in some cases relatively well and doesn't work so well on a larger scale. Even if we assume that we know what 'the main content of the word' is, languages distribute types of information encoded by linguistic units between roots and affixes in different ways. \n",
    "\n",
    "As an example, let's compare two words that have the same meaning: _zagryzt'_ (Russian) and _yat<sup>h</sup>a_ ([Lakota](https://en.wikipedia.org/wiki/Lakota_language)). Both mean 'gnaw; bite to death'; both contain an affix (more specifically, a prefix, see below), but the way this meaning is packaged in the word and distributed between the affix and the root is different:\n",
    "\n",
    "|     |     | \n",
    "|:---|:------:|\n",
    "|  Russian **_zagryzt'_**| 'gnaw; bite to death'  |\n",
    "| &nbsp;&nbsp;&nbsp;prefix _za-_ | 'completely; to death' |\n",
    "| &nbsp;&nbsp;&nbsp;root _gryzt'_ | 'bite' |\n",
    "|  Lakota **_yat<sup>h</sup>a_** | 'gnaw; bite to death'  |\n",
    "| &nbsp;&nbsp;&nbsp;prefix _ya-_ | 'cause something using teeth' |\n",
    "| &nbsp;&nbsp;&nbsp;root _t<sup>h</sup>a_ | 'die' |\n",
    "\n",
    "\n",
    "The part of meaning 'using teeth' is conveyed by the root in Russian but expressed in the prefix in Lakota. One should be careful with semantic definitions of morphological phenomena!\n",
    "\n",
    "Another distinction that is indirectly related to the previous two is **open-class** vs. **closed-class morphemes**. Morphemes can be grouped into classes depending on the typical positions they occupy in a sentence. Think of a context like _The cat was thinking about a \\_\\_\\__ -- what can occupy the slot after _a_? A lot of things, a potentially infinite list (_mouse_, _seagull_, _window_ etc.). What about a position here: _The cat was thinking about \\_\\_\\_ mouse_? Much fewer things can show up there: one of the articles -- _a_ or _the_; maybe a possessive pronoun.. This is the core of the difference between open-class and closed-class morphemes. Nominal roots are open-class morphemes (as well as verbs, adjectives and maybe something else), articles and other morphemes typically expressing types of meanings pre-defined by the grammar of the language are much less numerous, and not so open to extensions; they form **closed classes**. \n",
    "\n",
    "But, again, we should be careful not to mix up this distinction and the previous two: not all closed class morphemes are affixes! Some are more autonomous units -- clitics or even separate words:\n",
    "\n",
    "```{margin} \n",
    "Tagalog\n",
    "```\n",
    "<div id='outerTable'><table>\n",
    "  <tr><td>(1)&nbsp;&nbsp;&nbsp;&nbsp;</td><td><b>mga</b>&nbsp;&nbsp;&nbsp;&nbsp;</td><td>malalaking&nbsp;&nbsp;&nbsp;&nbsp;</td><td>saging</td></tr>\n",
    "  <tr><td></td><td><b>PL</b></td><td>big</td><td>banana</td></tr>\n",
    "  <tr><td></td><td colspan=3>'big banana<b>s</b>'</td></tr>\n",
    "</table></div>\n",
    "<br>\n",
    "\n",
    "So, the closed vs. open class distinction does not refer to how the morpheme behaves as part of the word and as part of the sentence. Rather, it describes how many other morphemes of this type the language has. These two things are often correlated, but, since their definitions are based on different aspects of language organization, one might expect that the boundaries they draw don't always align.\n",
    "\n",
    "I know.. We will just have to live with it and when using these terms, we need to make sure to communicate well what exactly we mean.\n",
    "\n",
    "### Derivational vs. inflectional morphology\n",
    "\n",
    "Yet another popular morphological distinction contrasts **derivational** and **inflectional** affixes. The intuition behind this distinction is that some affixes make new words out of whatever they attach to, while inflectional morphology just creates a variant of the same word. This intuition can be made precise in more than one way, depending on what you consider to be the defining criterion for whether two objects are similar enough to group them together as versions of each other. \n",
    "\n",
    "An often-cited criterion is that inflectional morphology does not change the word's part of speech. That's why _-er_ in _singer_ is a derivational morpheme: it makes a noun out of a verb (_sing_). Fair enough! The opposite does not necessarily hold though: some derivational affixes keep the part of speech the same: suffix _-ship_ attaches to nouns and produces nouns as well, for instance, _friendship_ is a noun and _friend_ is a noun as well. So, not messing with the part of speech is not enough for the affix to be inflectional. \n",
    "\n",
    "Another take on this distinction is 'derivational morphemes have clear semantic content', so that they significantly change the meaning of what they attach to. I don't think I have the energy to comment on that, we've said enough on semantic criteria on other occasions above. One thing to note is that I personally think that the meaning of the plural suffix _-s_ as in _cat**s**_-- an uncontroversially inflectional suffix -- is as clear or clearer than the contribution of the uncontroversially derivational suffix _-ness_ as in abstract properties like _tall**ness**_. I wouldn't rely on that too much.\n",
    "\n",
    "Finally, inflectional status of a morpheme can be approached via the effect that the grammatical pressure of the context has on the word. As one popular textbook puts it, 'inflectional morphemes represent relationships between different parts of a sentence'. A clear case is verbal agreement in English: in _Mary walk**s** fast_, the suffix _-s_ is brought to life by the fact that the sentence has a singular subject and the verb has to have the agreement suffix. This pressure makes the suffix inflectional, and _walk_ and _walks_ sort of **variants** of one and the same word -- in a more abstract sense of the notion **word** than before (you will sometimes find terms **lemma** or **lexeme** used to refer to this more abstract notion). This reasoning seems very attractive, but it seems a bit of a stretch to apply it to large classes of uncontroversially inflectional morphemes like past tense _-ed_, for instance. Nothing in the sentence forces you to use the past tense specifically -- you could've used the present tense and still get a well-formed sentence. Unless something more specific is said about where the intended meaning comes into play as 'part of the sentence', but it's not clear how to say something more specific on this. Tough!\n",
    "\n",
    "I think we will just have to agree with the underlying intuition (different words vs. variants of the same thing, a more abstract thing we can still call one and the same word) and think of different possible ways this can manifest itself in linguistic behaviour of different morphemes, but treat these behavioral tests with caution.\n",
    "\n",
    "We are now leaving the swampy territory of widespread but very complex morphological distinctions that mix together how morphemes can be used and what types of meanings they can express. Hooray!\n",
    "\n",
    "Now we will focus on affixes specifically and formal tools that languages use to attach them to wherever they need to be attached.\n",
    "\n",
    "### Positional types of affixes\n",
    "\n",
    "I will list the main formal types of affixes here. By 'formal' I mean that I will ignore the meanings that these affixes express, I will only be concerned with what you need to do to the sequence that the affix is being attached to and the sequence that corresponds to the affix itself so that they can be put together.\n",
    "\n",
    "`````{admonition} Oversimplification alert!\n",
    ":class: warning, dropdown\n",
    "Morphemes aren't always sequences! And most definitely not always continuous sequences. But more on that later. \n",
    "`````\n",
    "\n",
    "We will look at our classic types of affixes -- prefixes and suffixes -- but also ones that you might have not come across before: circumfixes, infixes, pattern morphology, as well as affixes that are better defined as operations rather than (continuous or discontinuous) sequences: reduplication, umlaut / ablaut, tone.\n",
    "\n",
    "\n",
    "#### Prefixes and suffixes\n",
    "\n",
    "Prefixes and postfixes are affixes distinguished by their linear position against their morphological base. **Prefixes** are attached to the beginning, **suffixes** (sometimes the term **postfix** is used synonymously) are attached to the end:\n",
    "\n",
    "\n",
    "|  Prefixes  |  Suffixes  |\n",
    "|:----------:|:----------:|\n",
    "| **a**moral     | read**able**   |\n",
    "| **in**accurate | moral**ize**   |\n",
    "| **il**legal    | friend**ship** |\n",
    "\n",
    "A word can have more than one prefix or affix at the same time:\n",
    "\n",
    "\n",
    "<center><big><span style=\"color:DeepPink\"><b>anti</b></span><span style=\"color:Indigo\"><b>dis</b></span><span style=\"color:Crimson\"><b>establish</b></span><span style=\"color:ForestGreen\"><b>ment</b></span><span style=\"color:DarkSlateGray\"><b>ari</b></span><span style=\"color:Purple\"><b>an</b></span><span style=\"color:SteelBlue\"><b>ism</b></span></big></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "Are the ones closer to the root still prefixes / suffixes? They are not in the very beginning or very end of the resulting word. Yes, they still are prefixes and suffixes. In order to make sense of it, it helps to think about morphological combination as happening in steps. These steps are not necessarily something that a speaker does sequentially in their head every time they produce a complex form like this (but also not necessarily not!) -- but it's a good way to represent the structure of complex words like these. At the point when a prefix or a suffix attaches, it attaches to the periphery of the word. But **then**, another prefix or suffix can attach, and it will be attaching to the beginning or end of the output of the previous step. So, affixes don't necessarily attach to roots directly, they attach to potentially bigger units, combinations of roots and other affixes. There is a term for that bigger unit, **stem** -- so, affixes attach to stems. Thinking about this as a multi-step process allows us to think about internal structure of complex words, where each step of conjoining an affix corresponds to conjoining two units together in a tree-like structure. \n",
    "\n",
    "Here is one piece of evidence for this hierarchical structure within words. Think about the word _unlockable_. You can understand this word in two different ways: \n",
    "\n",
    "1. something that can't be locked;\n",
    "2. something that can be unlocked.\n",
    "\n",
    "These two meanings fall out naturally if we think about two different processing that could've given rise to the same sequence _unlockable_:\n",
    "\n",
    "```{image} ./images/unlockable.png\n",
    ":alt: unlockable\n",
    ":class: bg-primary mb-1\n",
    ":width: 300px\n",
    ":align: center\n",
    "```\n",
    "\n",
    "#### Circumfixes\n",
    "\n",
    "A circumfix can best be seen as a combination of prefix and suffix frozen together. Historically, that's what they most often indeed are. \n",
    "\n",
    "Dutch and German are famous for having circumfixes as part of verbal morphology. For example, _dansen_ 'dance' and _horen_ 'hear' in analytic past tenses (a.k.a. present perfect and past perfect) show up in the form that involves circumfixes _ge- -t_ / _ge- -d_:\n",
    "\n",
    "> We hebben tot laat in de nacht **ge**dans**t**. <br>\n",
    "> Heb jij **ge**hoor**d** wat Marieke zei?\n",
    "\n",
    "#### Infixes\n",
    "\n",
    "Some affixes are not placed at the edges of the stem they attach to, but are insted inserted somewhere inside that stem. Existing cross-linguistic data suggests that even then, these affixes gravitate towards one of the edges of the stem: rules of their insertion are usually formulated in terms of the first syllable or the first consonant of the stem, or in terms of the last syllable or consonant. \n",
    "\n",
    "Infixes can be found, for instance, in Tagalog. _Kagat_ is a noun which means 'bite'; adding _-um-_ after the first consonant of the stem results in a past tense active verb _k\\<um\\>agat_ 'bit', while adding _-in-_ in that position produces a passive verb _k\\<in\\>agat_ 'was bit'. \n",
    "Interesting that when the stem starts with the vowel, the infix becomes a prefix in Tagalog: _awit_ 'song' ~ _um-awit_ 'sang'.\n",
    "\n",
    "Another example is [Ulwa](https://www.ethnologue.com/language/ulw/), where possessive morphemes are infixed in the root: _sú:lu_ 'dog', _sú:\\<ki\\>lu_ 'my dog', _sú:\\<ma\\>lu_ 'your dog', _sú:\\<ka\\>lu_ 'his/her dog'.\n",
    "\n",
    "\n",
    "#### Pattern (template) morphology\n",
    "\n",
    "An even less straighforward way of combining two morphemes is **pattern morphology** which Semitic languages (Arabic, Hebrew) are famous for. We have seen discontinuous affixes (circumfixes) and root that are interrupted by an affix (infix), but pattern morphology shows both these phenomena at the same time. A typical Arabic root is a template consisting of 3 consonants; affixes combining with these roots are discontinuous sequences of vowels that can be inserted between these consonants:\n",
    "\n",
    "```{margin} \n",
    "Figure from Islam, M.S., Masum, M.H., Bhuyan, M.S.I. and Ahmed, R., 2010. Arabic nominals in HPSG: a verbal noun perspective. 17th International Conference on HPSG (pp. 158-178). \n",
    "```\n",
    "```{margin} \n",
    "Now you see why abjad as a writing system makes sense for these languages -- consonants carry a lot of information that can't be deduced from the grammatical context; the rest can be left out without much loss.\n",
    "```\n",
    "```{image} ./images/ktb.png\n",
    ":alt: ktb\n",
    ":class: bg-primary mb-1\n",
    ":width: 400px\n",
    ":align: center\n",
    "```\n",
    "\n",
    "To get other meanings, for example, based on the root _k\\_t\\_b_, other vowels could be combined with the root. This is how you get _kitaab_ 'book', _kaatib_ 'writer' etc. \n",
    "\n",
    "\n",
    "\n",
    "`````{admonition} Extra info\n",
    ":class: tip, dropdown\n",
    "The distribution of prefixes, suffixes and discontinuous morphemes is not completely random in languages of the world. Here is a [linguistic universal](https://en.wikipedia.org/wiki/Greenberg%27s_linguistic_universals) by Joseph Greenberg that conditionally connects the presence of continuous and discontinuous affixes in a langauge:\n",
    "> If a language has discontinuous affixes then it also has either prefixing or suffixing or both.\n",
    "`````\n",
    "\n",
    "#### Affixes as operations\n",
    "\n",
    "##### Reduplication\n",
    "\n",
    "Sometimes, an affix adds segments to the stem but is not fully defined segmentally. In case of reduplication, for example, the general shape of the affix is all that is grammatically specified, but the actual segments that fill it in vary depending on the stem it attaches to. For instance, this is how you form a plural in [Ilocano](https://en.wikipedia.org/wiki/Ilocano_language): you take the first segments of the stem so that the result is a closed syllable, and attach it as a prefix: _kláse_ 'class' ~ _klas-kláse_ 'classes'. \n",
    "\n",
    "##### Umlaut, ablaut, tone and stress change\n",
    "\n",
    "Sometimes, an affix does not contain any segments at all and the only way to define it is as an operation producing some systematic alternation in the stem. Here are some types of those:\n",
    "\n",
    "- **Umlaut**: Fronting the vowel(s) in the stem. It's a term that mostly describes processes in Germanic languages, in particular, German, where fronting of a vowel is often used as a grammatical device, see _Mutter_ 'mother' vs. _Mütter_ 'mothers'. Occasionally, the same be found in English, e.g. _tooth_ vs. _teeth_.  There is no systematic of umlaut in Dutch as a grammatical device, but see, for instance, _stad_ 'city' vs. _steden_ 'cities'.\n",
    "- **Ablaut** is another Germanic vowel alternation term, not specifically fronting. Think _get:got_, _sing:sang_.\n",
    "- **Tone and stress change** can be grammatical instruments as well. In [Dida](https://en.wikipedia.org/wiki/Dida_language), a tone language spoken in Ivory Coast, changing the tone on a verb changes its tense interpretation: _li<sup>3</sup>_ 'ate' with tone 3 describes a one-time past event of eating, while _li<sup>2</sup>_ 'eat' describes a habitual event happening now from time to time. Sometimes, stress shift can serve the purposes of derivational morphology in English: compare _conrást_ (verb) and _cóntrast_ (noun).\n",
    "\n",
    "##### Suppletion\n",
    "\n",
    "Finally, the change that can happen to the stem when in a particular grammatical context can be as radical as a complete change. For instance, this is what happens to an English verb _go_ in the past tense, when it turns into _went_. Not only there is no segment corresponding to past tense, there is no clear rule on how _went_ derives from _go_ that could be formulated as a reasonable operation over segments. The same is true for Dutch _zijn_ 'be' vs. _was_ 'was'. It's just a completely different stem that expresses two things together: the lexical meaning of 'go' and the grammatical meaning of past tense.\n",
    "\n",
    "### Morphological typology\n",
    "\n",
    "We've seen examples from many different languages and many different types of morphemes. This is a good time to take a more systematic look at this landscape and organize it a little bit. Obviously, languages differ in what kind of morphological devices it uses and how much morphology in general its words contain. Let's take these two properties as two axes along which we can characterize languages. We will call the first one 'degree of allomorphy and fusion' and the second one 'degree of synthesis'.\n",
    "\n",
    "#### Allomorphy and fusion\n",
    "\n",
    "I started this part of the lecture with a sort of a morphological additive ideal, where clear segmental morphemes attach to each other in a predictable way, with each morpheme playing some clear grammatical and semantic role, something like this:\n",
    "\n",
    "|  Sg   |   Pl   |\n",
    "|:---:|:----:|\n",
    "| cat | cat**s** |\n",
    "| bike | bike**s** |\n",
    "\n",
    "There are many ways a language can deviate from this ideal. For example, a morpheme can change slighlty when combining with another morpheme. This happens with English plural _s_: in _cat**s**_ and _bike**s**_ it's pronounced as /s/, but in _dog**s**_, it's actually /z/ -- it's affected by the previous consonant. \n",
    "\n",
    "Turkic languages have **vowel harmony**: the plural suffix in Turkish, for example, has two **allomorphs**: _ler_ and _lar_. You have to choose one of them depending on the vowels in the stem you attach it to. Stems with front vowels (_e, i, ö, ü_) require _ler_, while stems with back vowels (_a, ı, o, u_) combine with _lar_. \n",
    "\n",
    "```{margin} \n",
    "Turkish\n",
    "```\n",
    "|  Sg   |   Pl   |\n",
    "|:---:|:----:|\n",
    "| _soru_ 'question'  | _soru**lar**_ |\n",
    "| _göz_ 'eye' | _göz**ler**_ |\n",
    "\n",
    "It's not always the affix that changes its shape depending on the stem: in Russian, the infinivie of 'write' is _pis-at'_ (where _-at'_ is the infinitive suffix) and the first person singular in present tense is _piš-u_ '(I) write', with a change in the final consonant of the stem. This latter type of alternation makes the morpheme boundary harder to draw even when the linear type of the affix is very simple: a prefix or a suffix. The morphemes kind of **fuse** together, affecting each other's appearance. We have seen different quite complicated linear types of morphemes above that can be very far from the additive ideal (infixes, templates, reduplication etc.), but take a moment to recognize that fusion can be seen as an additional parameter on top of that complexity. \n",
    "\n",
    "Languages differ in the levels of fusion that their morphology exhibits. The scale is from very little or zero fusion (languages like that are called **agglutinative**) to a lot of fusion and hard-to-find or barely existent morpheme boundaries (**fusional** languages).\n",
    "\n",
    "- Examples of agglutinative languages: Turkish, Finnish, Korean and others -- but even in those languages, some level of allomorphy is found (see the example of Turkish vowel harmony above; but also note that it does not affect morpheme boundaries). A typical agglutinative multi-morpheme word would be something like _ev-ler-iniz-den_ 'from your houses', translated morpheme-by-morpheme as 'house-plural-your(plural)-from'.\n",
    "\n",
    "- Examples of fusional languages: a lot of Indo-European languages (but not English! and not Dutch); Russian; Spanish; Semitic languages (but that's subject to debate). One example here is the Russian word _lun-a_, which encodes the nominative case singular with one undivisible suffix _-a_, and -- even more compactly -- expresses the genitive plural of the same word as _lun_, which is completely undivisible. \n",
    "\n",
    "#### Degree of synthesis\n",
    "\n",
    "Languages differ also in how many morphemes a word in this language typically has. The continuum here is from isolating languages (roughly one morpheme per word) to polysynthetic languages with typically extreme amount of information packed in one word by means of a large number of morphemes.\n",
    "\n",
    "- Examples of isolating languages: Chinese, Vietnamese, Thai, English.\n",
    "\n",
    "```{margin} \n",
    "Thai ([source](https://journals.sagepub.com/doi/pdf/10.1177/0539018410389107))\n",
    "```\n",
    "<div id='outerTable'><table>\n",
    "  <tr><td>(2)&nbsp;&nbsp;&nbsp;&nbsp;</td><td>cháaŋ&nbsp;&nbsp;&nbsp;</td><td>ŋuaŋ&nbsp;&nbsp;&nbsp;</td><td>yaaw&nbsp;&nbsp;&nbsp;</td></tr>\n",
    "  <tr><td></td><td>elephant</td><td>tusk</td><td>long</td></tr>\n",
    "  <tr><td></td><td colspan=3>'The elephant has long tasks'</td></tr>\n",
    "</table></div>\n",
    "<br>\n",
    "\n",
    "- Examples of polysynthetic languages: Chukchi, Ainu, Greenlandic, Norwest Caucasian such as Adyghe, where the verb morphologically expresses a lot of information about the event it describes:\n",
    "\n",
    "```{margin} \n",
    "Temirgoy Adyghe ([corpus](http://adyghe.web-corpora.net/adyghe_corpus/search)).\n",
    "```\n",
    "```{margin} \n",
    "I thank Peter Arkadiev for helping me with this example and the calculations below.\n",
    "```\n",
    "\n",
    "<div id='outerTable'><table>\n",
    "  <tr><td>(3)&nbsp;&nbsp;&nbsp;&nbsp;</td><td>∅-qə-p-fe-t-ṣ̂ə-ŝʷə-šʼt-ep</td></tr>\n",
    "  <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;</td><td>3.ABS-CSL-2SG.IO-BEN-1PL.ERG-do-HBL-FUT-NEG</td></tr>\n",
    "  <tr><td>&nbsp;&nbsp;&nbsp;&nbsp;</td><td colspan=1>'We won’t be able to do it for you.'</td></tr>\n",
    "</table></div>\n",
    "<br>\n",
    "\n",
    "In English, there is no single word that can contain all that information: English doesn't have the morphological devices to do so. Inflectional morphology of English verbs produces very few forms that could be seen as belonging to one and the same lemma. Given how few forms English words have, it might seem reasonable to not even treat combinations of morphemes as a result of some actual process, as we did above. Do speakers need to build together different forms of the verb like _walk_ from pieces if instead they can simply remember the forms _walking_, _walked_, _walks_ as they are? \n",
    "\n",
    "|  Option 1   |   Option 2   |    |\n",
    "|---:|:----|:----|\n",
    "| _walks_ |      | + _s_ |\n",
    "| _walked_ | _walk_   | + _ed_ |\n",
    "| _walking_ |    | + _ing_ |\n",
    "\n",
    "\n",
    "If one can try to make sense of this first option for English, it's definitely not viable for languages with more complex morphology: it's simply impossible to memorize all the different forms of one word of the type shown in (3). If we try to evaluate how many different forms this verbs can have, it's 246 person-number combinations for agreement with different participants, and multiplying this by different other options of filling in other positions, we get **177120** forms! And this is just a modest estimation. Morphology has to involve operations rather than direct memorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interlinear glossing\n",
    "\n",
    "\n",
    "`````{admonition} External content\n",
    ":class: danger\n",
    "\n",
    "I presented some examples above in a 3-line format that we haven't properly introduced before, such as this Tagalog example:\n",
    "\n",
    "<div id='outerTable'><table>\n",
    "  <tr><td>(1)&nbsp;&nbsp;&nbsp;&nbsp;</td><td><b>mga</b>&nbsp;&nbsp;&nbsp;&nbsp;</td><td>malalaking&nbsp;&nbsp;&nbsp;&nbsp;</td><td>saging</td></tr>\n",
    "  <tr><td></td><td><b>PL</b></td><td>big</td><td>banana</td></tr>\n",
    "  <tr><td></td><td colspan=3>'big banana<b>s</b>'</td></tr>\n",
    "</table></div>\n",
    "<br>\n",
    "\n",
    "This format is called **interlinear glossing**. It's concisely described in [the document on Leipzig glossing conventions](https://www.eva.mpg.de/lingua/pdf/Glossing-Rules.pdf).\n",
    "`````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morphology and language technology\n",
    "\n",
    "\n",
    "### Classic tasks\n",
    "\n",
    "Morphology and language technology interact in cases where word structure matters for the NLP task (either is part of the task directly or helps it in some way) and, vice versa, where language technology helps identify word structure. These are not two necessarily separate cases -- as we will see below, often it's both things at the same time. \n",
    "\n",
    "```{margin} \n",
    "Starting now, I will include small bits of Python code in the course notes so that you can get used to them little by little and try them out on your own in a Colab or locally on your computer, if you like (some of the things I try here will require installing packages and downloading models for them to work -- if you are trying something and it doesn't work immediately, check out documentation of the package). If you don't want to dive deep into the code parts now, feel free to ignore them and just focus on the general story.\n",
    "```\n",
    "Some classic NLP tasks that have morphology at their core involve abstracting away from some morphological properties of the words in text and representing these words at a more abstract level. One such task is **lemmatization**: given a word with some inflectional morphology, lemmatization outputs its 'basic' form taken as the representative of all the forms of the word, like in this code snippet below, where lemmatization is part of the analysis performed by the spaCy library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute --> compute\n",
      "computed --> compute\n",
      "computing --> compute\n",
      "computer --> computer\n",
      "computers --> computer\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "for token in ['compute', 'computed', 'computing', 'computer', 'computers']:\n",
    "    print(token + ' --> ' + sp(token)[0].lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another version of the task of abstracting away from inflectional morphology is **stemming**: instead of mapping a word to its 'basic' form, stemming chops off parts of the word that are probably not part of its stem or root. You see that the result (here using another popular NLP toolkit -- NLTK) is different from what we got with lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute --> comput\n",
      "computed --> comput\n",
      "computing --> comput\n",
      "computer --> comput\n",
      "computers --> comput\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "for token in ['compute', 'computed', 'computing', 'computer', 'computers']:\n",
    "    print(token + ' --> ' + stemmer.stem(token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both lemmatization and stemming have the same purpose: abstracting away from grammatical details, which allows to focus on the core lexical content of the text. When does it matter? Well, whenever you think that grammar matters less than words! If you have a model that relies on word counts, for example, you might want to count more abstract entities than those that are actually found in text. If you care whether something has to do with computers and/or computing, you might not necessarily want to count _compute_, _computing_ etc. separately. Might be a good idea for topic detection, for instance -- but it really depends on how you want to approach the task at hand.\n",
    "\n",
    "The opposite task -- generating a form with the required inflectional morphology given the 'basic' form -- can also be important for certain applications. One example is automatic extension of search queries: if the search query contains _compute_, it might be a good idea to match documents that contain _computing_, _computed_ etc. Here a system that can do automatic **inflection** can help. Here are two examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speak + indicative past tense --> spoke\n"
     ]
    }
   ],
   "source": [
    "from mlconjug3 import Conjugator\n",
    "conjugator = Conjugator('en')\n",
    "verb = conjugator.conjugate(\"speak\")\n",
    "print('speak' + ' + indicative past tense --> ' + verb['indicative']['indicative past tense']['I'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tooth + plural --> teeth\n"
     ]
    }
   ],
   "source": [
    "import inflect\n",
    "p = inflect.engine()\n",
    "print('tooth' + ' + plural --> ' + p.plural_noun('tooth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources, groups, events\n",
    "\n",
    "The tasks described above might be almost trivial for languages like English, which doesn't have much (inflectional) morphology, but are way harder for languages with rich morphology, like some of the languages we talked about in this class. \n",
    "\n",
    "- [UniMorph](https://unimorph.github.io/) is a collaborative project that aims to improve how NLP handles complex morphology in the world’s languages. It focusses on creation of morphologically annotated datasets and the standards for such datasets that can, in turn, be used to create new models that deal with complex morphology better.\n",
    "- [SIGMORPHON](https://sigmorphon.github.io/) is the ACL Special Interest Group on Computational Morphology and Phonology. The group organizes SIGMORPHON workshops that are a platform for new research on computational morphology and phonology as well as shared tasks -- challenges for morphological and phonological automatic analysis. This year's two SIGMORPHON challenges were:\n",
    "1. [Automatic Interlinear Glossing](https://github.com/sigmorphon/2023GlossingST) that we discussed earlier in this class. Formally, the task is defined as a sequence-to-sequence task, where a sentence and its translation are given as input and the output should contain the gloss layer (morpheme-by-morpheme translation). The usefulness of this task for linguists that have to deal with a lot of manual glossing work is obvious -- but a sort of a side-effect of this task is a system that has learned some aspects of morphology of the target language, and we can examine what it learned successfully, what was difficult to learn, and try to figure out why.\n",
    "    \n",
    "```{margin} \n",
    "Gitksan\n",
    "```\n",
    "<div id='outerTable'><table>\n",
    "  <tr><td><i>(Input 1) Source</i>: </td><td>Ii</td><td>k̲'ap&nbsp;&nbsp;&nbsp;</td><td>g̲aniwila</td><td>yukwhl</td><td>surveyors</td></tr>\n",
    "  <tr><td><i><b>(Output) Gloss</i></b>: </td><td>CCNJ&nbsp;&nbsp;&nbsp;</td><td>VER</td><td>continually-MANR&nbsp;&nbsp;&nbsp;</td><td>do-CN&nbsp;&nbsp;&nbsp;</td><td>surveyors</td></tr>\n",
    "  <tr><td><i>(Input2) Translation:</i>&nbsp;&nbsp;&nbsp; </td><td colspan=5>‘But the surveyors continued.’</td></tr>\n",
    "</table></div>\n",
    "\n",
    "<br>\n",
    "\n",
    "2. [Morphological Inflection Generation](https://github.com/sigmorphon/2023InflectionST), examples of which we just saw above: Given the 'basic' form of the word and the grammatical characteristics of the desired output, the system needs to produce the required form:\n",
    "\n",
    "```{margin} \n",
    "Turkish\n",
    "```\n",
    "```\n",
    "asimptot\tN;NOM(PL;PSS(1,PL))\tasimptotlarımız\n",
    "```\n",
    "\n",
    "- [Universal Dependencies](https://universaldependencies.org/) is a framework and a huge dataset with grammatical annotation for over 100 languages. The annotation includes both morphology and syntax -- which we will discuss next week! Here is one sentence from the dataset, in the CONLL format, where each word of the text is characterized grammatically in a separate line:\n",
    "\n",
    "```{margin} \n",
    "Croatian\n",
    "```\n",
    "```\n",
    "# text = Kazna medijskom mogulu obnovila raspravu u Makedoniji\n",
    "1\tKazna\tkazna\tNOUN\tNcfsn\tCase=Nom|Gender=Fem|Number=Sing\t4\tnsubj\t_\t_\n",
    "2\tmedijskom\tmedijski\tADJ\tAgpmsdy\tCase=Dat|Definite=Def|Degree=Pos|Gender=Masc|Number=Sing\t3\tamod\t_\t_\n",
    "3\tmogulu\tmogul\tNOUN\tNcmsd\tCase=Dat|Gender=Masc|Number=Sing\t1\tnmod\t_\t_\n",
    "4\tobnovila\tobnoviti\tVERB\tVmp-sf\tGender=Fem|Number=Sing|Tense=Past|VerbForm=Part|Voice=Act\t0\troot\t_\t_\n",
    "5\traspravu\trasprava\tNOUN\tNcfsa\tCase=Acc|Gender=Fem|Number=Sing\t4\tobj\t_\t_\n",
    "6\tu\tu\tADP\tSl\tCase=Loc\t7\tcase\t_\t_\n",
    "7\tMakedoniji\tMakedonija\tPROPN\tNpfsl\tCase=Loc|Gender=Fem|Number=Sing\t4\tobl\t_\t_\n",
    "```\n",
    "\n",
    "As you see, grammatical information is assigned to the word as a whole, and not strictly speaking **glossed** -- there is no division of the original word into morphemes and morphological information is not ordered accoding to how it is expressed in the word. But it is still rich and useful information, so I think you should know Universal Dependencies as a source of morphological information!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and morphology\n",
    "\n",
    "A lot of the large deep learning models that we see around today and that have shown impressive performance on language-related tasks do not employ any analysis of morphology explicitly. In fact, they don't operate on the level of individual characters, individual words or individual morphemes at all. They do something else. Before doing any deep learning magic to the input text, they split it into smaller bits -- tokens -- a process known as **tokenization**. More often than not, current models use [**subword tokenization**](https://huggingface.co/docs/transformers/tokenizer_summary#subword-tokenization), which means that the tokens they are dealing with are usually somewhat smaller than words. What these pieces should be is decided by a separate training process that looks for regularities in the character combinations in training data. There are different ways of finding these regularities and the resulting pieces, and these different algorithms together with different training data and different desireable size of the resulting vocabulary of tokens can produce different splits into tokens. As a linguist, I am always curious to inspect the resulting vocabularies of tokens to see whether -- and to what extent -- they end up close to what we think the morphological composition of words is! Sometimes it's pretty close, sometimes not. \n",
    "\n",
    "Let's see for ourselves! Remember the sentence we started today's lecture with? Here it is again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Juniper's mother-in-law is kinda hilarious, isn't she?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize it using a couple of different subword tokenizers that pre-process text for a couple of popular models. \n",
    "\n",
    "The first one is the tokenizer used by GPT2 -- a predecessor of newer GPT-based models, including ChatGPT, which is much more recent, but AFAIK still uses exactly the same tokenizer as GPT2 (you can compare the result below with what the [OpenAI tokenization demo](https://platform.openai.com/tokenizer) outputs).\n",
    "\n",
    "The other tokenizer we can look at is built to work with BERT, another popular relatively recent language model.\n",
    "\n",
    "Both tokenizers are of the subword kind, built for English on English data, with some differences in the tokenization algorithm and training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 tokenizer:\n",
      "Jun | iper | 's | mother | - | in | - | law | is | kinda | hilarious | , | isn | 't | she | ?\n",
      "\n",
      "BERT tokenizer:\n",
      "Jun | iper | ' | s | mother | - | in | - | law | is | kinda | hi | lar | ious | , | isn | ' | t | she | ?\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "print('GPT2 tokenizer:')\n",
    "print(' | '.join([x.strip('Ġ') for x in gpt_tokenizer.tokenize(sentence)])+'\\n')\n",
    "print('BERT tokenizer:')\n",
    "print(' | '.join([x.strip('##') for x in bert_tokenizer.tokenize(sentence)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the results are generally similar -- but also a bit different, especially when it comes to somewhat more rare words, such as the rare name _Juniper_ and adjective _hilarious_.\n",
    "\n",
    "It's also interesting to see how subword tokenization changes when the tokenizer is trained on many languages at the same time. Here are two examples from multilingual (100+ languages) tokenizers when they are given the same sentence: very frequent English words are tokens again (raising the suspicion that English was prevalent in the training data) but the bits are generally shorter, which makes sense as they will need to be shared across all languages of the model. Check out how the Multilingual BERT tokenizer finds _us_ in _hilarious_ - an actual English free morpheme, first person plural pronoun, but not really a morpheme in this particular word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilingual BERT tokenizer:\n",
      "Juni | per | ' | s | mother | - | in | - | law | is | kind | a | hil | ario | us | , | isn | ' | t | she | ?\n",
      "\n",
      "XLM-RoBERTa tokenizer:\n",
      "Juni | per | ' | s | mother | - | in | - | law | is | kind | a |  | hila | rious | , | isn | ' | t | she | ?\n"
     ]
    }
   ],
   "source": [
    "mbert_tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "xlmr_tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "print('Multilingual BERT tokenizer:')\n",
    "print(' | '.join([x.strip('##') for x in mbert_tokenizer.tokenize(sentence)])+'\\n')\n",
    "print('XLM-RoBERTa tokenizer:')\n",
    "print(' | '.join([x.strip('▁') for x in xlmr_tokenizer.tokenize(sentence)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good moment to look back at the homework assignment from last week: when formulating an alphabet-related task, I mentioned that ChatGPT is [not great at character-level tasks](https://chat.openai.com/share/436f6fd5-78e7-40ca-83a9-11eae9c997dd). Now you understand why: it does not work with characters, but instead with larger units -- subword tokens -- that are treated basically as undivisible monoliths. Given this, it's in fact surprising how **good** it is with some of those tasks!\n",
    "\n",
    "Before we leave this topic, it's worth mentioning models with **character-level tokenizers**. They are less widespread, but very interesting! Here is how one of such tokenizers deals with our running example -- indeed, each symbol is a separate token: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J | u | n | i | p | e | r | ' | s |   | m | o | t | h | e | r | - | i | n | - | l | a | w |   | i\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('google/byt5-base')\n",
    "print(' | '.join(tokenizer.tokenize(\"Juniper's mother-in-law is kinda hilarious, isn't she?\")[:25]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not talk much about character-level models and their pros and cons, but, again, linguistically it's extremely interesting whether and how these models learn to group these character-level tokens together while being trained on some linguistic task such as translation or next-token prediction. Are there any traces of these models operating on morphological units such as morphemes or words? \n",
    "\n",
    "Maybe! Here are two figures that suggest they might be. \n",
    "\n",
    "\n",
    "\n",
    "```{margin} \n",
    "Hahn, M. and Baroni, M. 2019. Tabula nearly rasa: Probing the linguistic knowledge of character-level neural language models trained on unsegmented text. TACL 7. [[pdf](https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00283/1923543/tacl_a_00283.pdf)]\n",
    "```\n",
    "This first figure shows activations of a component in a character-level neural network that was trained on text **with spaces between words deleted**. So, the model didn't have information about word or morpheme boundaries during training. Still, the model developed dedicated units that track positions that correspond to word boundaries in all three languages under consideration experiments (ground truth word boundaries are marked with green lines). Moreover, some of the model's boundaries that were found correspond to morpheme boundaries: _co-_ and _produced_ in _co-produced_ are treated as separate elements, and a weaker boundary is found after the prefix _pro-_. The German word _Hauptaufgabe_ ('main task') is segmented into the morphemes _haupt_, _auf_ and _gabe_. For the details about training and results, see the paper.\n",
    "\n",
    "\n",
    "```{image} ./images/tabula.png\n",
    ":alt: words\n",
    ":class: bg-primary mb-1\n",
    ":width: 650px\n",
    ":align: center\n",
    "```\n",
    "```{margin} \n",
    "Edman, L., Toral, A. and van Noord, G. 2023. Are Character-level Translations Worth the Wait? An Extensive Comparison of Character-and Subword-level Models for Machine Translation. [arXiv preprint](https://arxiv.org/abs/2302.14220).\n",
    "```\n",
    "Another piece of evidence pointing in the direction of the usefullness of morphological units for practical linguistic tasks comes from work on character-level translation done here at GroNLP. The graph below shows the dynamics of the relative importance of source and target sequences for the translation (here, from German to English). Peaks in source importance at the beginning of each word suggest word-level modeling: the processed is packaged in words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{image} ./images/byt5.png\n",
    ":alt: words\n",
    ":class: bg-primary mb-1\n",
    ":width: 600px\n",
    ":align: center\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`````{admonition} Homework 4\n",
    ":class: note\n",
    "**Task 1**\n",
    "\n",
    "Read the following introduction to morphology:\n",
    "\n",
    "> Fromkin, Rodman and Hyams.2017. An Introduction to Language, 11th edition. Chapter 2 'The Words of Language', pp. 33-60.\n",
    "\n",
    "Submit, as before, three things discussed in this chapter that were either not discussed during the lecture or discussed differently. Say a bit more than just naming these things -- provide a short comparison or definition.\n",
    "\n",
    "NB: Pay attention to exercises at the end of the chapter -- something like that will appear in the final exam! But more on this later in the course.\n",
    "\n",
    "\n",
    "**Task 2**\n",
    "\n",
    "Familiarize youself with [Leipzig glossing rules](https://www.eva.mpg.de/lingua/pdf/Glossing-Rules.pdf). Gloss the following simplified excerpt from the syllabus for this course. Make sure to do it sentence-by-sentence, adhering to the rules as much as possible. List points where you had doubts along the way. \n",
    "\n",
    "> De informatiewetenschap vereist dat beoefenaars op het gebied van taaltechnologie daadwerkelijk over taalkundige kennis beschikken. Deze kennis moet worden afgestemd op de uitdagingen van natuurlijke taalverwerking. Dit vereist praktische kennis van de basisconcepten van de taaltheorie.\n",
    "`````"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ling_course",
   "language": "python",
   "name": "ling_course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
